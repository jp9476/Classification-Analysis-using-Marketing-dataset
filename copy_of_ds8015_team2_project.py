# -*- coding: utf-8 -*-
"""Copy of DS8015_Team2_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vFZjx085GPl1HVYRRwji_tAQ0mfVxvfU

## Details of used techniques or models for this project using 4/10 Lectures (as asked in rubric)


1.   Week 4: Python Third Party Library (**Pandas**, **Numpy**, **Sklearn**)
2. Week 5:  EDA (**Missing DATA**, **Categorical Data**, Distribution analysis using **Visualization**, **Descriptive Statistic**)
3. Week 6: Introduction to ML (**Classification** Analysis, **Feature selection**, Model selection)
4. Week 7: End to end Machine Learning **label encoding** and **Dummy encoding**, **10-fold cross validation**, ensemble methods)
5. Week 10: **Classification** Analysis and **models**(**Decision tree, KNN, Logictic Regression**)
6. Week 11: **PCA** ( Feature selection, Standardization) 
7. Week 12: Neural Network (**MLP** model)

# Download the CSV
"""

from google.colab import files
files.upload()

"""# Libraries"""

# imported useful libraries 
# for models mostly used sklearn lib

import os
import numpy as np
import pandas as pd 

from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report,r2_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn import model_selection
from sklearn.ensemble import BaggingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA 

import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('seaborn')

import warnings
from imblearn.over_sampling import SMOTE
warnings.filterwarnings('ignore')

"""# Attributes description (1 points)"""

bank_df = pd.read_csv('bank-additional-full.csv', sep = ';')
bank_df.head(5)

"""#Statistics of the data (use the tools learnt in this course to generate the data statistics)"""

bank_df.describe()

"""#Data cleaning and Preprocessing"""

bank_df.info()

"""# [How] did you clean up data? (1 point)"""

bank_df.isnull().sum()

"""Note: There is no null values in the dataset"""

#removal of duplicate rows
print(bank_df.shape)
bank_df = bank_df.drop_duplicates() 
print(bank_df.shape)

"""Note: There are 12 rows duplicate rows, which are removed"""

#Checking Unique values
bank_df.nunique()

"""Note:
columns age, duration, campaign, pdays have lots of unique values.
"""

# Finding the unique values in each attributes
for y in bank_df.columns:
    if bank_df[y].dtype == 'object':
        print(f'Name: {y}\n unique values: {bank_df[y].unique()}')
        print('--------------------------------------------------')

# making a copy of df
bank_df2 = bank_df.copy()
bank_df2.head(5)

"""# Data Visualization (Extra)

## Histogram

Note: histogram for some selected attributes which shows the distribution of a numeric varables such as age, campaign and so on.
"""

bank_df.hist(bins=50, figsize=(20,15))
plt.show()

"""## Bar Graph"""

# count positive and negative response of members for each category
sns.countplot(x="poutcome", hue="y",data=bank_df, palette="pink")

# Count of Education distribution
figure, ax1 = plt.subplots()
figure.set_size_inches(19, 8)
sns.countplot(x = bank_df['education'])
ax1.set_xlabel('education', fontsize=12, color = 'g', rotation = 0)
ax1.set_ylabel('Count', fontsize=12, color = 'g', rotation = 45)
ax1.set_title('Count of Education distribution', fontsize=15, color ='red')
ax1.tick_params(labelsize=11)
sns.despine()

"""## Pie Chart

Note: shwing pie chart for only two selected columns.
"""

# count for maritial status wise distribution - pie chart
a1 = bank_df['marital'].value_counts().plot(kind='pie')
a1.set_title("Count Marital Status wise distribution")

a4 = bank_df['y'].value_counts().plot(kind='pie')
a4.set_title("Count of Dependent(Result) Attribute distribution")

"""#  Data Transformation"""

# Label encoder order is alphabetical
labelencoder_X = LabelEncoder()
bank_df['job']      = labelencoder_X.fit_transform(bank_df['job']) 
bank_df['marital']  = labelencoder_X.fit_transform(bank_df['marital']) 
bank_df['education']= labelencoder_X.fit_transform(bank_df['education']) 
bank_df['default']  = labelencoder_X.fit_transform(bank_df['default']) 
bank_df['housing']  = labelencoder_X.fit_transform(bank_df['housing']) 
bank_df['loan']     = labelencoder_X.fit_transform(bank_df['loan']) 
bank_df['contact']     = labelencoder_X.fit_transform(bank_df['contact']) 
bank_df['month']       = labelencoder_X.fit_transform(bank_df['month']) 
bank_df['day_of_week'] = labelencoder_X.fit_transform(bank_df['day_of_week']) 
bank_df['poutcome'].replace(['nonexistent', 'failure', 'success'], [1,2,3], inplace  = True)

#converted age into group category
def age(df):
    df.loc[df['age'] <= 35, 'age'] = 1
    df.loc[(df['age'] > 35) & (df['age'] <= 45), 'age'] = 2
    df.loc[(df['age'] > 45) & (df['age'] <= 70), 'age'] = 3
    df.loc[(df['age'] > 70), 'age'] = 4
           
    return df

age(bank_df);

# convert duration in some parts and label those 
def duration(df):

    df.loc[df['duration'] <= 100, 'duration'] = 1
    df.loc[(df['duration'] > 100) & (df['duration'] <= 175)  , 'duration']    = 2
    df.loc[(df['duration'] > 175) & (df['duration'] <= 325)  , 'duration']   = 3
    df.loc[(df['duration'] > 325) & (df['duration'] <= 640), 'duration'] = 4
    df.loc[df['duration']  > 640, 'duration'] = 5
    return df
duration(bank_df);

bank_df
newfinal_df = bank_df.copy() # make a copy 
newfinal_df.head(5)

"""# Which attributes are used, which ones are eliminated? Why? (1 points)"""

#selection of independendent variable. We are building the model that predict the category of people who are most likely to subscribe term deposit. For that we need the attributes before contact.
newfinal_df = newfinal_df.drop(["duration", "contact","month","day_of_week",'y'], axis=1)
newfinal_df.info()

"""
#Machine learning Models
#d) Solution description (12 points: 3 points per technic/model used in the solution)"""

#Converting dependent variable categorical to dummy
y = pd.get_dummies(bank_df['y'], columns = ['y'], prefix = ['y'], drop_first = True)
y

X_train, X_test, y_train, y_test = train_test_split(newfinal_df, y, test_size = 0.20, random_state = 101)
k_fold = KFold(n_splits=10, shuffle=True, random_state=0)

X_train.head()

# perform scaling for training data
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

#X_train

"""## Model 1 Gaussian NB"""

#Model 1 Gaussian Naive Bayes 
GNB_model = GaussianNB().fit(X_train, y_train)
Predt_GNB = GNB_model.predict(X_test)
print('Confusion Matrix:')
print(confusion_matrix(y_test, Predt_GNB))
print('Accuracy for Gaussian NB:',round(accuracy_score(y_test, Predt_GNB),2)*100)
GNB_CV = (cross_val_score(GNB_model, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())

"""## Model 2 Decision Tree"""

#Model 2 Decison Tree
dt_model = DecisionTreeClassifier(criterion='gini').fit(X_train, y_train)
pred_DT = dt_model.predict(X_test)
print('Confusion Matrix:')
print(confusion_matrix(y_test, pred_DT))
DTree_CV = (cross_val_score(dt_model, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())
print('Accuracy for Decision Tree:',round(accuracy_score(y_test, pred_DT),2)*100)

"""## Model 3 KNN

"""

#Model 3 KNN
X_train1, X_test1, y_train1, y_test1 = train_test_split(newfinal_df, y, test_size = 0.2, random_state = 101)

knn_acr = []
from sklearn import metrics
for i in range(1,50):
    KNN_nr = KNeighborsClassifier(n_neighbors = i).fit(X_train1,y_train1)
    KNN_predict = KNN_nr.predict(X_test1)
    knn_acr.append(metrics.accuracy_score(y_test1, KNN_predict))
    
plt.figure(figsize=(10,6))
plt.plot(range(1,50),knn_acr,color = 'green',linestyle='--', 
         marker='d',markerfacecolor='blue', markersize=10)
plt.title('K value vs. Accuracy')
plt.xlabel('K value')
plt.ylabel('Accuracy')
print("The highest accuracy is:",max(knn_acr)," and the optimum number of K is:",knn_acr.index(max(knn_acr)))

#Model 3 KNN
k = 40  # from above selection
model_KNN = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
Pred_KNN = model_KNN.predict(X_test)
print("For k = 40, Accuracy for KNN: ", round(metrics.accuracy_score(y_test, Pred_KNN),4)*100,"%")
print('Confusion Matrix:')
print(confusion_matrix(y_test, Pred_KNN))
KNNModel_CV = (cross_val_score(model_KNN, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())

"""## Model 4 Logistic Regression"""

#Model 4 Logistic Regression
logistic_model = LogisticRegression().fit(X_train,y_train)
pred_logi = logistic_model.predict(X_test)
print('Confusion Matrix:')
print(confusion_matrix(y_test, pred_logi))
print('Accuracy for Logistic Regression:',round(accuracy_score(y_test, pred_logi),2)*100)
LogiModel_CV = (cross_val_score(logistic_model, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())

"""## Model 5 MLP"""

# PCA method for feature reduction
# we learned it in a class
pca = PCA(n_components=0.95)
pca.fit(X_train)
PCA_X_train = pca.transform(X_train)
PCA_X_test = pca.transform(X_test)

# define and train an MLPClassifier named mlp on the given data
ml = MLPClassifier(hidden_layer_sizes=(50,100,50), max_iter=250, activation='relu', solver='adam',learning_rate_init=0.001, random_state=1)
ml.fit(PCA_X_train, y_train)

print('Accuracy for the MLP:',ml.score(PCA_X_test, y_test))
# draw the confusion matrix
predict_ml = ml.predict(PCA_X_test)
confusion_matrix_ml = confusion_matrix(y_test, predict_ml)
print('Confusion matrix:')
print(confusion_matrix_ml)

# print the training error and MSE
print('Training error: %f' % ml.loss_curve_[-1])
a_mlp = accuracy_score(y_test, predict_ml)
print('Accuracy:',accuracy_score(y_test, predict_ml))
print('MSE: %f' % mean_squared_error(y_test, predict_ml))

"""# Comparison of Model

## Accuracy wise comparison
"""

Class_model = pd.DataFrame({'Models Name': ['Decision Tree','K-Near Neighbors', 'Gausian Naive Bayes', 'Logistic Regression','MLP'],
                            'Accuracy':  [DTree_CV, KNNModel_CV, GNB_CV, LogiModel_CV, a_mlp]})
Class_model.sort_values(by='Accuracy', ascending=False)

"""Note: the logistic regression gives highest accuracy and gausian NB gives lowest accuracy result.

## ROC Curves
"""

fig, axis_a1 = plt.subplots(nrows = 2, ncols = 2, figsize = (16,13))

#--------------------GAUSSIAN Naive Bayes (GNB Model) ---------------------
model1 = GNB_model.predict_proba(X_test)
prdt_m1 = model1[:,1]
GAUS_fp, GAUS_tp, thresholdgau = metrics.roc_curve(y_test, prdt_m1)
GAUS_roc = metrics.auc(GAUS_fp, GAUS_tp)

axis_a1[0,0].plot(GAUS_fp, GAUS_tp, 'purple', label = 'AUC = %0.2f' % GAUS_roc)
axis_a1[0,0].plot([0, 1], [0, 1],'grey', linestyle='--')
axis_a1[0,0].set_ylabel('True Positive Rate (TP Rate)',fontsize=16)
axis_a1[0,0].set_xlabel('False Positive Rate (FP Rate)',fontsize=13)
axis_a1[0,0].set_title('Receiver Operating Characteristic(ROC) for Gaussian NB',fontsize=16)
axis_a1[0,0].legend(loc = 'Best', prop={'size': 14})
##########################################################################################################################
#--------------------Decision Tree Model --------------------
model2 = dt_model.predict_proba(X_test)
prdt_m2 = model2[:,1]
DT_fp, DT_tp, thresholdrfc = metrics.roc_curve(y_test, prdt_m2)
RF_roc = metrics.auc(DT_fp, DT_tp)

axis_a1[0,1].plot(DT_fp, DT_tp, 'purple', label = 'AUC = %0.2f' % RF_roc)
axis_a1[0,1].plot([0, 1], [0, 1],'grey', linestyle='--')
axis_a1[0,1].set_xlabel('False Positive Rate (FP Rate)',fontsize=13)
axis_a1[0,1].set_ylabel('True Positive Rate (TP Rate)',fontsize=16)
axis_a1[0,1].set_title('ROC for Decision Tree (DT)',fontsize=16)
axis_a1[0,1].legend(loc = 'Best', prop={'size': 14})
##########################################################################################################################
#--------------------K Nearest Neighbor (KNN) Model----------------------
model3 = KNN_nr.predict_proba(X_test)
prdt_m3 = model3[:,1]
KNNR_fp, KNNR_tp, thresholdknn = metrics.roc_curve(y_test, prdt_m3)
KNNR_roc = metrics.auc(KNNR_fp, KNNR_tp)

axis_a1[1,0].plot(KNNR_fp, KNNR_tp, 'purple', label = 'AUC = %0.2f' % KNNR_roc)
axis_a1[1,0].plot([0, 1], [0, 1],'grey', linestyle='--')
axis_a1[1,0].set_ylabel('True Positive Rate (TP Rate)',fontsize=16)
axis_a1[1,0].set_xlabel('False Positive Rate (FP Rate)',fontsize=13)
axis_a1[1,0].legend(loc = 'Best', prop={'size': 14})
axis_a1[1,0].set_title('ROC for KNN Model',fontsize=16)
###########################################################################################################################
#--------------------Logistic Regression Model--------------------
model4 = logistic_model.predict_proba(X_test)
prdt_m4 = model4[:,1]
LogiR_fp, LogiR_tp, thresholdlog = metrics.roc_curve(y_test, prdt_m4)
Log_roc = metrics.auc(LogiR_fp, LogiR_tp)

axis_a1[1,1].plot(LogiR_fp, LogiR_tp, 'purple', label = 'AUC = %0.2f' % Log_roc)
axis_a1[1,1].plot([0, 1], [0, 1],'grey', linestyle='--')
axis_a1[1,1].set_title('ROC for Logistic Regression Model',fontsize=16)
axis_a1[1,1].set_ylabel('True Positive Rate (TP Rate)',fontsize=16)
axis_a1[1,1].set_xlabel('False Positive Rate (FP Rate)',fontsize=13)
axis_a1[1,1].legend(loc = 'Best', prop={'size': 14})

plt.subplots_adjust(wspace=0.16)
plt.tight_layout()

# Comparing Roc Curves
plt.plot([0, 1], [0, 1], linestyle='--', color = 'black')
plt.plot(LogiR_fp, LogiR_tp, linestyle='-',color='green', label='Logistic Regression')
plt.plot(GAUS_fp, GAUS_tp, linestyle='-', color='red', label='Gaussian NB')
plt.plot(KNNR_fp, KNNR_tp, linestyle='-', color='blue', label='KNN')
plt.plot(DT_fp, DT_tp, linestyle='-',color='orange', label='Decsion Tree Model')

#For title of the Plot
plt.title('ROC curve')
#For x and y labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

plt.legend(loc = 'lower right', prop={'size': 15})
plt.show();

"""## Classification Reports (Precision, Recall and F1-score)"""

print('\nClassification Report for KNN (k Nearest Neighbor)\n\n',classification_report(y_test, Pred_KNN))

print('\nClassification Report for Decision Tree model\n\n',classification_report(y_test, pred_DT))

print('\nClassification Report for Gaussian Naive Bayes Model\n\n',classification_report(y_test, Predt_GNB))

print('\nClassification Report for Logistic Regression Model\n\n',classification_report(y_test, pred_logi))